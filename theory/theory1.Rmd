---
title: "Homework 1.1"
author: "Eric Brewster"
date: "July 7, 2024"
output:
  pdf_document:
    extra_dependencies: amsmath
  html_document:
    df_print: paged
---
 
```{r setup, include=FALSE}
library(formatR)
knitr::opts_chunk$set(echo = TRUE, eval=TRUE, tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

# Problem 1: 8.8

## Part a: Which of these estimators are unbiased?

Textbook definition 4.11 states that random variable Y is said to have an exponential distribution with parameter beta > 0 if and only if the density function of Y is
\[
f(y) =
\begin{cases} 
\frac{1}{beta} e^{-y/beta} & \text{for } 0 \leq y < \infty, \\
0 & \text{elsewhere}.
\end{cases}
\]
In the problem definition, beta is represented by theta. Textbook theorem 4.10 states that if Y is an exponential random variable with parameter beta, then 
\[
\mu = E(Y) = beta
\]

Because E(Y) is equal to theta (which is equal to beta), each sample of Y is equal to theta; that is,
\[
E(Y_1) = \theta, \quad E(Y_2) = \theta, \quad E(Y_3) = \theta
\]

An estimator of theta is unbiased if the expected value of the estimator is equal to theta.

1.
\[
E(\hat{\theta}_1) = E(Y_1) = \theta
\]
As stated above, the expected value of \[E(Y_1)\] is equal to theta, so this estimator is unbiased.

2.
\[
E(\hat{\theta}_2) = E\left(\frac{Y_1 + Y_2}{2}\right) = \frac{1}{2} E(Y_1) + \frac{1}{2} E(Y_2) = \frac{1}{2} \theta + \frac{1}{2} \theta = \theta
\]
Textbook theorem 4.5.2 states that E[cg(Y)] = cE[g(Y)] where c is a constant and g(y) is a function of a continuous random variable, and theorem 4.5.3 states E[g1(Y)+g2(Y)+· · ·+gk(Y)] = E[g1(Y)]+E[g2(Y)]+· · ·+E[gk(Y)] given the same conditions. Using theorem 2, you move c = 1/2 before the expectation; from there, theorem 3 allows you to separate the expectation of samples 1 and 2. Because the expectations of samples 1 and 2 are equal to theta, the estimator is equal to theta, so this estimator is unbiased.

3.
\[
E(\hat{\theta}_3) = E\left(\frac{Y_1 + 2Y_2}{3}\right) = \frac{1}{3} E(Y_1) + \frac{2}{3} E(Y_2) = \frac{1}{3} \theta + \frac{2}{3} \theta = \theta
\]
This estimator follows similar logic to estimator 2: use theorem 4.5.3 to separate the terms and theorem 4.5.2 to bring the constant to the front. Converting the expectations of samples 1 and 2 to theta and adding equals theta, so this estimator is also unbiased.

4.
\[
E(\hat{\theta}_4) = E(\min(Y_1, Y_2, Y_3)) = \min(E(Y_1), E(Y_2), E(Y_3)) = \min(\theta, \theta, \theta) = \theta
\]
The expectation of the minimum of three samples returns one value (expectation of sample 1, sample 2, or sample 3). Therefore, you can rewrite the expression to be the minimum of the expectations of the samples. Given that each sample's expectation is theta, the minimum of (theta, theta, theta) is equal to theta, so this estimator is unbiased.

5.
\[
E(\bar{Y}) = E\left(\frac{Y_1 + Y_2 + Y_3}{3}\right) = \frac{1}{3} E(Y_1) + \frac{1}{3} E(Y_2) + \frac{1}{3} E(Y_3) = \frac{1}{3} \theta + \frac{1}{3} \theta + \frac{1}{3} \theta = \theta
\]
Following the logic from estimators 2 and 3, use theorem 4.5.3 to separate the terms and theorem 4.5.2 to bring the constant forward. Adding the terms together equals theta, so this estimator is unbiased.


## Part b: Among the unbiased estimators, which has the smallest variance?

The variance in an exponential distribution is equal to the parameter squared. Because V(Y) equals theta^2, each sample of V(Y) equals theta^2; that is, 
\[
V(Y_1) = \theta^2, \quad V(Y_2) = \theta^2, \quad V(Y_3) = \theta^2
\]

Determine the variance of each estimator:

1.
\[
V(\hat{\theta}_1) = V(Y_1) = \theta^2
\]
As stated above, the expected value of \[V(Y_1)\] is equal to theta squared, so this estimator is unbiased.

2.
\[
V(\hat{\theta}_2) = V\left(\frac{Y_1 + Y_2}{2}\right) = \frac{1}{2^2} V(Y_1) + \frac{1}{2^2} V(Y_2) = \frac{1}{4} \theta^2 + \frac{1}{4} \theta^2 = \frac{1}{2} \theta^2
\]
This follows the same logic as the expectation of the mean from part 1 and estimator 2, although one change is necessary. When bringing the constant c in front of the variance symbol, you square the constant, as expected variance is squared.

3.
\[
V(\hat{\theta}_3) = V\left(\frac{Y_1 + 2Y_2}{3}\right) = \frac{1}{3^2} V(Y_1) + \frac{2^2}{3^2} V(Y_2) = \frac{1}{9} \theta^2 + \frac{4}{9} \theta^2 = \frac{5}{9} \theta^2
\]
Following the same logic as estimator 2, you have to square the constant when pulling it out of the variance. In this case, this results in a slightly larger variance for estimator 3.

4.
\[
V(\hat{\theta}_4) = V(\min(Y_1, Y_2, Y_3)) = \min(V(Y_1), V(Y_2), V(Y_3)) = \min(\theta^2, \theta^2, \theta^2) = \theta^2
\]
This follows the same logic as estimator 4 in part a: the expected variance of the minimum of three samples is equal to the minimum of the variances for the samples. As each expected variance is theta squared, the minimum value is theta squared.

5.
\[
V(\bar{Y}) = V\left(\frac{Y_1 + Y_2 + Y_3}{3}\right) = \frac{1}{3^2} V(Y_1) + \frac{1}{3^2} V(Y_2) + \frac{1}{3^2} V(Y_3) = \frac{1}{9} \theta^2 + \frac{1}{9} \theta^2 + \frac{1}{9} \theta^2 = \frac{1}{3} \theta^2
\]
Following the logic from estimators 2 and 3, pull out the constant in each term and square it. Adding these together returns the lowest variance of the unbiased estimators.

Estimator 5, the sample mean of Y, has the lowest variance of the estimators.

# Problem 2

## Part a: Show that \(\hat{\theta} = \bar{Y}\) is unbiased for \(\theta\), and calculate its MSE.

To show that the estimator is unbiased, its expectation must be equal to theta.The first step is to substitute in the definition of Y bar, the sample mean
\[
E(\bar{Y}) = E\left(\frac{1}{n} \sum_{i=1}^{n} Y_i\right)
\]

From here, pull out the constant 1/n and alter the numerator by the logic that the expectation for the sum of samples is equal to the sum of the expectations of each sample.

\[
E(\bar{Y}) = \frac{\sum_{i=1}^{n} E(Y_i)}{n}
\]

As the expectation of each sample is equal to theta, the numerator functions as the summation of theta from 1 to n. Therefore, the numerator can be changed to n times theta.
\[
\frac{n \theta}{n}
\]

Cancelling out the n's returns theta, showing that the estimator is equal to theta, thus being unbiased. As the estimator is unbiased, the MSE of the estimator is equal to the variance of the estimator.
\[
V(\bar{Y}) = V\left(\frac{1}{n} \sum_{i=1}^{n} Y_i\right)
\]

Following the same logic as above, pull the 1/n constant out of variance; however, accounting for variance means squaring the constant, so the denominator becomes n squared. The numerator is the variance of the summation of each term, which converts to the summation of the variances of each term from 1 to n. As the variances are equal to theta squared, the numerator converts to n times theta squared.

\[
\frac{1}{n^2} V\left(\sum_{i=1}^{n} Y_i\right) = \frac{\sum_{i=1}^{n} V(Y_i)}{n^2}
= \frac{n \theta^2}{n^2} = \frac{\theta^2}{n}
\]

Cancelling out a n returns the MSE of the estimator, theta squared divided by n.

## Part b: Consider the alternative estimator for theta, calculate this estimator’s bias and MSE.

The bias of the estimator is the expectation of the estimator minus theta.

\[
E\left(\hat{\theta}_2\right) = \frac{E\left(1 + \sum_{i=1}^{n} Y_i\right)}{n+1} = \frac{1 + \sum_{i=1}^{n} E(Y_i)}{n+1} = \frac{1 + n\theta}{n+1}
\]

In the first step, you plug in the new estimator, recalling that E(c) = c so the denominator is equal to n plus 1. Using the same logic, apply the expectation to both terms in the numerator, and the 1 remains as 1. Moreover, the expectation for the summation of samples is equal to the summation from 1 to n of the expectation of each sample. As the expectation for each sample is theta, n times theta replaces the summation, leaving the result.

Now knowing the expectation for the estimator, you can subtract theta to get the bias term shown below.

\[
\frac{1 + n\theta}{n+1} - \theta = \frac{1 + n\theta}{n+1} - \frac{(n+1)\theta}{n+1}
= \frac{1-\theta}{n+1}
\]

The MSE of the new estimator can be found by adding variance with bias squared.
\[
\frac{V\left(1 + \sum_{i=1}^{n} Y_i\right)}{(n+1)^2} + \frac{(1 - \theta)^2}{(n+1)^2}
= \frac{1 + n\theta^2}{(n+1)^2} + \frac{(1-\theta)^2}{(n+1)^2}
= \frac{(1+n\theta^2)(1-\theta)^2}{(n+1)^2}
\]
The right side of the expression is the bias term found above squared. The left side uses the principle from part a of pulling constant out and squaring them, hence why you square the denominator. Also shown in part a is the logic that the variance of the summation of terms is equal to the summation of variances, which equals n times theta squared. Algebraic work reveals the final MSE.

## Part c: Find a scenario where \(\hat{\theta}\) is better than \(\hat{\theta}_2\) and a scenario where \(\hat{\theta}_2\) is better than \(\hat{\theta}\), where "better" is decided based on MSE.

The first estimator will be better (lower MSE) when theta is very small compared to n or n is very large; choosing 5 for n and 0.2 for theta returns an MSE of 0.008 for estimator 1 and about 0.0213 for estimator 2. The second estimator will be better when theta is large and/or n is small; plugging in 3 for n and 0.9 for theta gives and MSE of 0.27 for estimator 1 and about 0.002 for estimator 2.

## Part d: Suppose that instead we want to estimate the variance \(\theta^2\). What estimator from class can we use if we want our estimator to be unbiased?

The estimator from class to use to estimate variance in an unbiased manner assuming an unknown mean is 
\[
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (Y_i - \bar{Y})^2 = \frac{n}{n-1} \hat{\sigma}^2
\]

because the expectation of s squared is equal to sigma squared as shown below.

\[
E(s^2) = \frac{n}{n-1} E(\hat{\sigma}^2) = \frac{n}{n-1} \frac{n-1}{n} \sigma^2 = \sigma^2
\]

## Part e: Consider instead the estimator \((\bar{Y}^2)\) for \(\theta^2\). Is this estimator unbiased? If not, how can we change it to make it unbiased?

\[
E(\bar{Y}^2) = V(\bar{Y}) + (E(\bar{Y}))^2
\]

Using  textbook theorem 3.6, expand the estimator into more known terms. Part a of problem 2 showed that V(Y) is equal to theta squared over n, and the expectation of the mean is equal to theta in the problem definition.

\[
\frac{\theta^2}{n} + \theta^2 = \frac{\theta^2}{n} + \frac{n \theta^2}{n} = \theta^2 \left(\frac{n+1}{n}\right)
\]

Because the estimator's squared expectation is not equal to theta squared, the estimator is biased. To make the estimator unbiased, you need to multiply the result above by \[n/(n+1)\], thereby also inserting the term into the original estimator. The unbiased estimator becomes: 
\[
\frac{n}{n+1} \, \bar{Y}^2
\]

# Problem 3: 8.13

## Part a: Show that the suggested estimator is a biased estimator of V(Y).

Textbook theorem (and proof) 3.7 proves that the variance of a binomial distribution Y is equal to npq, where n is the number of trials and p is the probability of success. To show that the suggested estimator is biased, the expected value of the estimator must be equal to npq.

\[
E\left(n \left(\frac{Y}{n}\right) \left(1 - \frac{Y}{n}\right)\right) = nE\left(\frac{Y}{n} - \frac{Y^2}{n^2}\right) = nE\left(\frac{Y}{n}\right) - nE\left(\frac{Y^2}{n^2}\right) = np - \frac{1}{n}E(Y^2)
\]

The first step uses theorem 3.4 which states E[cg(Y)] = cE[g(Y)] for discrete random variables; the constant n, or the number of trials, moves in front of the expectation term. Multiplying the inside of the expectation precedes splitting the expression into two terms, possible via theorem 3.5 (ability to split terms for discrete random variables). The final step is two-fold: substitute in p (given that Y/n is an unbiased estimator for p) and move the constant outside of the expectation term (using theorem 3.4). 

\[
np - \frac{1}{n} E(Y^2)
\] 

Converting the second term uses theorem 3.6: 

\[
\sigma^2 = E(Y^2) - 2\mu^2 + \mu^2 = E(Y^2) - \mu^2
\] 

Rearranging this formula:

\[
E(Y^2) = \sigma^2 + \mu^2
\] 

Recalling that sigma squared, or V(Y), equals npq and mu, or E(Y), equals np:

\[
E(Y^2) = V(Y) + (E(Y))^2 = np(1 - p) + (np)^2 = np(1 - p) + n^2 p^2
\]

Using algebra,
\[
np - \frac{1}{n} (np(1 - p) + n^2 p^2) = np - p(1 - p) - np^2 = np(1 - p) - p(1 - p)
\]

The expected value of the given estimator is not equal to npq (the pq term is the difference), so the suggested estimator is biased.


## Part b: Modify n(Y/n)(1 - Y/n) slightly to form an unbiased estimator of V(Y).

From part a, the expected value of the suggested estimator (using algebra to reconstruct),
\[
np(1 - p) - p(1 - p) = (n - 1)p(1 - p) = (n - 1)pq
\]

Multiplying the result by \[n/(n-1)\] returns the desired npq, so multiply the suggested estimator by this term:

\[
E\left( \frac{n}{n-1} n \left( \frac{Y}{n} \right) \left(1 - \frac{Y}{n}\right) \right)
= \frac{n}{n-1} E\left( n \left( \frac{Y}{n} \right) \left(1 - \frac{Y}{n}\right) \right)
= \frac{n}{n-1} np(1 - p)
= np(1 - p)
\]

The logic is to pull out the constant \[n/(n-1)\] and substitute in the known expectation to end at npq (after algebraic maneuvers). Since the expectation of the new estimator is equal to V(Y), 

\[
\frac{n}{n-1} n \left( \frac{Y}{n} \right) \left(1 - \frac{Y}{n}\right) = \frac{n^2}{n-1} nY \left(1 - \frac{Y}{n}\right)
\]
is an unbiased estimator.


