---
title: "HW2STA4273"
author: "Eric Brewster"
date: "2025-02-21"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 6.1

Monte Carlo estimation for this problem is:
$$
(b-a)\int_a^b g(x)dx = \frac{\pi}{3}(\frac{1}{m}\sum_{i=1}^m sin(u))
$$

```{r 6.1}
m <- 10000
x <- runif(m, 0, pi/3) # u above comes from Uniform(0, pi/3)
theta.hat <- pi/3 * mean(sin(x)) # sum of sin(x) divided by m samples, multiplied by pi/3

y <- replicate(1000, expr = {x <- runif(m, 0, pi/3) # run 1000 times
theta.hat <- pi/3 * mean(sin(x))})

mean(y)
sd(y)
```
Actual value of the integral is:
$$
\int_0^{\pi/3} sin(t) = -cos(t)|_0^{\pi/3} = -cos(\pi/3) + cos(0) = -1/2 + 1 = 1/2
$$
Running the simulation 1000 times gives a mean of y close to the 0.5 but not exact. The standard deviation of y tends to be about 0.0027.


# Problem 6.3

Monte Carlo estimation for this problem is:
$$
(b-a)\int_a^b g(x)dx = \frac{1}{2}(\frac{1}{m}\sum_{i=1}^m e^{-u})
$$

True value of integral:
$$
\int_0^{0.5} e^{-x}dx = -e^{-x}|_0^{0.5} = -e^{-0.5} + e^0 = 1 - e^{-0.5} \approx 0.3934693
$$

```{r 6.3.1}
m <- 10000
u <- runif(m, 0, 0.5) # sampling u from uniform(0, 0.5)
theta <- 0.5 * mean(exp(-u))
theta

est <- replicate(1000, expr = { # run 1000 times
u <- runif(m, 0, 0.5)
theta <- 0.5 * mean(exp(-u))})

mean(est)
var(est)
```
Mean of sampling from Uniform(0, 0.5) is nearly the same as true value of integral, estimated variance around 3.252896e-07.


Now sample from exponential distribution calculating mean of samples less than 0.5; that is,
$$
\hat{\theta^*} = \frac{1}{m}\sum_{i=1}^m (v < 0.5)
$$


```{r 6.3.2}
m <- 10000
v <- rexp(m, 1) # generating from exponential distribution
theta <- mean(v <= 0.5) 
theta

est1 <- replicate(1000, expr = {
 v <- rexp(m, 1)
 theta <- mean(v <= 0.5)})

mean(est1)
c(var(est1), sd(est1))

var(est)/var(est1)
```
As the ratio is well below 1, the variance from the standard exponential distribution's sampling is greater than the variance from the uniform distribution's sampling. In the attached pdf of handwritten work, I show the calculus/algebra behind the ratio and why the variance from the uniform sampling is smaller. This work uses the formulas on page 153 of the Rizzo book. The variance of the uniform distribution's sampling is smaller because uniform sampling covers the range more evenly whereas exponential sampling skews towards low x-values.

# Problem 6.4

This uses the code snippet from example 3.8, chapter on generating random variables.
Logic: If U ~ Gamma(r, lambda) and V ~ Gamma(s, lambda) are independent, then 
$$
X = \frac{U}{U + V}
$$
has the Beta(r, s) distribution needed for this problem. 
Steps for generating random Beta(a, b) variates:
1. Generate a random u from Gamma(a, 1).
2. Generate a random v from Gamma(b, 1).
3. Deliver x = u/(u+v)


```{r 6.4}
ratioGammas <- function(x, a, b) {
  u <- rgamma(10000, a, 1)
  v <- rgamma(10000, b, 1)
  y <- u/(u+v)
  return(mean(y <= x))
}

x <- seq(0.1, 0.9, 0.1)
k <- length(x)
p <- numeric(k)

for(i in 1:k) p[i] <- ratioGammas(x[i], 3, 3)
round(rbind(x, pbeta(x, 3, 3), p), 3) # compare to pbeta
```
Monte Carlo estimates (represented by p) are very close to the exact values (generated by pbeta, middle row).


# Problem 6.13

Need to find two importance functions f1 and f2 that are close to g(x) and supported on (1, inf); that is, random variables pulled from distribution will always be greater than 1. For a normal distribution to be supported on (1, inf), center on x=1 by folding- double the density to ignore the negative values. This is similar to a chi-square distribution with 1 degree of freedom; the chi-square with df=1 is the square of standard normal distribution (similar shape).
For the gamma f2, the distribution is shifted to be centered on x=1. The alpha value is 3/2: too low (I tried 1/2 and 1) skewed the distribution too far right, and too high (I tried 5/2 and 4) made the distribution too symmetric. 3/2 matches the shape well. For the beta value, 2 more closely replicated the rate of decay in g(x).

```{r 6.13.1}
x <- seq(1, 10, 0.01)
y <- x^2 * exp(-x^2/2)/sqrt(2 * pi) # g(x) function given in the problem
plot(x, y, type = "l", ylim = c(0, 1))

lines(x, 2 * dnorm(x, 1), lty = 2) # folded normal distribution
lines(x, dgamma(x - 1, 3/2, 2), lty = 3) # shifted gamma distribution
legend("topright", legend = c("g(x)", "f1", "f2"), lty = 1:3)
```

Now, need to compare the ratios of g(x)/f1(x) and g(x)/f2(x)
```{r 6.13.2}
plot(x, y/(dgamma(x - 1, 3/2, 2)), type = "l", lty = 1, ylab = "ratio")
lines(x, y/(2 * dnorm(x, 1)), lty = 2)
legend("topright", legend = c("f1", "f2"), lty = 1:2)
```
Importance function f2, the folded normal distribution, should produce a smaller variance because the ratio is closer to a constant function.


# Problem 6.14

Using f1 and f2 from problem 6.13,

```{r 6.14}
m <- 10000

is1 <- replicate(1000, expr = {
 x <- sqrt(rchisq(m, 1)) + 1
 f <- 2 * dnorm(x, 1)
 g <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
 mean(g/f)
 })

is2 <- replicate(1000, expr = {
 x <- rgamma(m, 3/2, 2) + 1
 f <- dgamma(x - 1, 3/2, 2)
 g <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
 mean(g/f)
 })

c(mean(is1), mean(is2))

c(var(is1), var(is2))

var(is1)/var(is2)
```

Given the ratio is less than 1, the shifted normal distribution is more efficient than the shifted gamma.


