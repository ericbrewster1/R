---
title: "HW4STA4273"
author: "Eric Brewster"
date: "2025-04-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Problem 11.3: Metropolis-Hastings sampling

Using example 11.1,
$r(x_t, y) = \frac{f(y) \, g(x_t \mid y)}{f(x_t) \, g(y \mid x_t)} = \frac{(1 + x_t^2)\pi}{(1 + y^2)\pi} \cdot \frac{\sqrt{2\pi\sigma} \, e^{-\frac{(x_t - y)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma} \, e^{-\frac{(y - x_t)^2}{2\sigma^2}}} = \frac{1 + x_t^2}{1 + y^2}$

This uses the normal N(u_t, sigma^2) proposal distribution - X_t is the previous value in the chain. For MH, (1) start at x[1]; (2) sample from rnorm; (3) decide if new value fits cauchy shape better, if yes then move to new value; (4) repeat



```{r 11.3}
m <- 10000
sigma <- 3
x <- numeric(m)
x[1] <- rnorm(1, 0, sigma) # start somewhere random in the normal distribution
k <- 0
u <- runif(m)
for (i in 2:m) { 
 xt <- x[i - 1] 
 y <- rnorm(1, xt, sigma) # new value
 num <- 1 + xt^2 
 den <- 1 + y^2
 num <- num * dnorm(xt, y)
 den <- den * dnorm(y, xt)
 if (u[i] <= num/den) { # if less than or equal to cauchy criteria, accept value
  x[i] <- y
 }
 else {
  x[i] <- xt
  k <- k + 1
 }
}
print(k)

p <- seq(0.1, 0.9, 0.1) # primary deciles
burn <- 1000
xBurned <- x[(burn + 1):m]
Q <- quantile(xBurned, p)
round(rbind(Q, qcauchy(p)), 3)

p <- seq(0.95, 1, 0.01) # upper tail
Q <- quantile(xBurned, p)
round(rbind(Q, qcauchy(p)), 3)

p <- ppoints(100) # qqplot
Q <- quantile(xBurned, p)
z <- qcauchy(p)
qqplot(z, Q, cex = 0.5)
abline(0, 1)
```
The 10-90 deciles of the observations are close to the standard Cauchy distribution, but these values differ greatly in the upper tail / 5% of values. This is visible in the QQ plot with the increasingly far from the line points.



# Problem 11.6: Random walk Metropolis

Laplace density:
$$
f(x) = \frac{1}{2}e^{-|x|}
$$
so
$$
r(x_t, y) = \frac{f(y)}{f(x_t)} = \frac{e^{-|y|}}{e^{-|x_t|}} = e^{|x_t|-|y|}
$$
Start at x0, walk the distance of sigma (standard deviation of proposal), propose new point and accept or reject based on the criteria above. More probable means accept. The proposed points come from the normal N(u_t, sigma^2) distribution, similar to problem 11.3. The return of the function is the chain x and the number of rejected points. 



```{r 11.6}
randomWalkLaplace <- function(N, x0, sigma) {
 x <- numeric(N)
 x[1] <- x0
 u <- runif(N)
 k<-0
 for (i in 2:N) {
 xt <- x[i - 1]
 y <- rnorm(1, xt, sigma)
 if (u[i] <= exp(abs(xt) - abs(y)))
 x[i] <- y
 else {
 x[i] <- x[i - 1]
 k <- k + 1
 }

 }
 return(list(x = x, k = k))
 }
N <- 5000
sigma <- c(0.5, 1, 2, 4)
x0 <- rnorm(1)
rw1 <- randomWalkLaplace(N, x0, sigma[1])
rw2 <- randomWalkLaplace(N, x0, sigma[2])
rw3 <- randomWalkLaplace(N, x0, sigma[3])
rw4 <- randomWalkLaplace(N, x0, sigma[4])
print(c(rw1$k, rw2$k, rw3$k, rw4$k))

cat("rejection rates ", (c(rw1$k, rw2$k, rw3$k, rw4$k)/N),
 "\n")

b <- 100
y1 <- rw1$x[(b + 1):N]
y2 <- rw2$x[(b + 1):N]
y3 <- rw3$x[(b + 1):N]
y4 <- rw4$x[(b + 1):N]

par(mfrow = c(2, 2))
plot(rw1$x, type = "l")
plot(rw2$x, type = "l")
plot(rw3$x, type = "l")
plot(rw4$x, type = "l")
par(mfrow = c(1, 1))

par(mfrow = c(2, 2))
p <- ppoints(200)
y <- qexp(p, 1)
z <- c(-rev(y), y)
fx <- 0.5 * exp(-abs(z))
hist(y1, breaks = "Scott", freq = FALSE, ylim = c(0,
 0.5))
lines(z, fx)
hist(y2, breaks = "Scott", freq = FALSE, ylim = c(0,
 0.5))
lines(z, fx)
hist(y3, breaks = "Scott", freq = FALSE, ylim = c(0,
 0.5))
lines(z, fx)
hist(y4, breaks = "Scott", freq = FALSE, ylim = c(0,
 0.5))
lines(z, fx)
par(mfrow = c(1, 1))

par(mfrow = c(2, 2))
Q1 <- quantile(y1, p)
qqplot(z, Q1, cex = 0.4)
abline(0, 1)
Q2 <- quantile(y2, p)
qqplot(z, Q2, cex = 0.4)
abline(0, 1)
Q3 <- quantile(y3, p)
qqplot(z, Q3, cex = 0.4)
abline(0, 1)
Q4 <- quantile(y4, p)
qqplot(z, Q4, cex = 0.4)
abline(0, 1)
par(mfrow = c(1, 1))
```

Comparing the chains using different variances, each of the chains appears to be matched to the overlayed laplace distribution. The qq plots show that the chains of sigma values 1 and 2 fit best, with the second chain (sigma = 1) more efficient - that is, a better acceptance rate.


# Problem 11.10: Gibbs sampler/ beta and binomial

Gibbs sampler steps: 
1. Generate X*(t) from binomial
2. Update x(t)
3. Generate Y*(t) from beta using x(t)
4. Set (X(t), Y(t)) = new values

```{r 11.10}
N <- 10000
burn <- 2000
a <- 2
b <- 3
n <- 10
x <- y <- rep(0, N)
x[1] <- rbinom(1, prob = 0.5, size = n)
y[1] <- rbeta(1, x[1] + a, n - x[1] + b)
for (i in 2:N) {
 x[i] <- rbinom(1, prob = y[i - 1], size = n)
 y[i] <- rbeta(1, x[i] + a, n - x[i] + b)
 }
xBurned <- x[(burn + 1):N]
f1 <- table(xBurned)/length(xBurned)

i <- 0:n
fx <- choose(n, i) * beta(i + a, n - i + b)/beta(a, b)
round(rbind(f1, fx), 3)

barplot(fx, space = 0, ylim = c(0, 0.15), xlab = "n",
 main = "p(n)=bar; est=points")
points(0:n + 0.5, f1)
```

The table and graph above compare the estimated distribution with the true PMF for the beta-binomial shown below.

$$
f(x) = \binom{n}{x} \frac{1}{B(a, b)B(x + a, n - x + b)}
$$

From the visuals above, the probabilities match closely.




